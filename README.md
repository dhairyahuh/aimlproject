# ASD/ADHD Voice Detection System

## Project Overview

Real-time differential diagnosis system for **Autism Spectrum Disorder (ASD)** vs **Attention-Deficit/Hyperactivity Disorder (ADHD)** using acoustic and prosodic feature analysis with **Multi-Layer Perceptron (MLP)** neural networks.

### Key Features

âœ… **Automated Feature Extraction** - 100+ acoustic and prosodic features  
âœ… **Real-time Recording & Inference** - 5-second voice sample analysis  
âœ… **MLP Classifier** - 3-layer neural network (128-64-32 neurons)  
âœ… **3-Class Classification** - ASD, ADHD, or Healthy  
âœ… **Confidence Scores** - Probability estimates for each class  
âœ… **Vocal Marker Explanation** - Interpretable feedback on key indicators  
âœ… **Streamlit Dashboard** - Interactive web-based UI  
âœ… **Cross-Validation** - 5-fold CV with comprehensive metrics  

---

## System Architecture

```
ASD_ADHD_Detection/
â”‚
â”œâ”€â”€ config/                      # Configuration management
â”‚   â””â”€â”€ config.py               # Centralized configuration (50+ parameters)
â”‚
â”œâ”€â”€ src/                         # Source code modules
â”‚   â”œâ”€â”€ feature_extraction/     # Acoustic & prosodic feature extraction
â”‚   â”‚   â”œâ”€â”€ mfcc_extractor.py
â”‚   â”‚   â”œâ”€â”€ spectral_extractor.py
â”‚   â”‚   â””â”€â”€ prosodic_extractor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                 # MLP model architecture
â”‚   â”‚   â”œâ”€â”€ mlp_classifier.py
â”‚   â”‚   â””â”€â”€ model_utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/          # Data preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ audio_preprocessor.py
â”‚   â”‚   â”œâ”€â”€ feature_normalizer.py
â”‚   â”‚   â””â”€â”€ data_augmentation.py
â”‚   â”‚
â”‚   â”œâ”€â”€ evaluation/             # Evaluation & metrics
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ cross_validation.py
â”‚   â”‚   â””â”€â”€ visualization.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                  # Utility functions
â”‚       â”œâ”€â”€ logger.py
â”‚       â”œâ”€â”€ file_handler.py
â”‚       â””â”€â”€ constants.py
â”‚
â”œâ”€â”€ data/                        # Data management
â”‚   â”œâ”€â”€ raw/                    # Original audio files
â”‚   â”œâ”€â”€ processed/              # Extracted features
â”‚   â””â”€â”€ splits/                 # Train/Val/Test splits
â”‚
â”œâ”€â”€ models/                      # Model storage
â”‚   â”œâ”€â”€ saved/                  # Trained model checkpoints
â”‚   â””â”€â”€ checkpoints/            # Training checkpoints
â”‚
â”œâ”€â”€ streamlit_app/              # Web dashboard
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ pages/
â”‚   â””â”€â”€ assets/
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ 01_feature_extraction.ipynb
â”‚   â”œâ”€â”€ 02_model_training.ipynb
â”‚   â”œâ”€â”€ 03_evaluation.ipynb
â”‚   â””â”€â”€ 04_realtime_demo.ipynb
â”‚
â”œâ”€â”€ results/                     # Experiment results & plots
â”œâ”€â”€ logs/                        # Training & inference logs
â”œâ”€â”€ tests/                       # Unit tests
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

---

## Feature Engineering

### 1. **MFCC Features** (52 features)
- **13** Mel-Frequency Cepstral Coefficients
- **13** Delta features (velocity)
- **13** Delta-Delta features (acceleration)
- **13** Statistics per feature type (mean, std, min, max)

*Based on: `python_speech_features/` and `librosa`*

### 2. **Spectral Features** (24 features)
- Spectral Centroid, Rolloff, Bandwidth
- Zero-Crossing Rate (ZCR)
- RMS Energy, Log Energy
- Chroma features (12-dimensional)
- Statistics: mean, std, min, max

*Based on: `pyAudioAnalysis/`*

### 3. **Prosodic Features** (30+ features)
- **F0/Pitch** - Mean, Std, Range, Median, Coefficient of Variation
- **Formants** - F1, F2, F3 with statistics
- **Jitter** - Fundamental frequency perturbation (ASD marker)
- **Shimmer** - Amplitude perturbation (ASD marker)
- **HNR** - Harmonic-to-Noise Ratio
- **Voice Quality** - Voice breaks, voiced rate

*Based on: `Parselmouth/` and `Dinstein-Lab/ASDSpeech/` (49 autism features)*

### 4. **Total Feature Dimension**
- **MFCC**: 52
- **Spectral**: 24  
- **Prosodic**: 30+
- **Total**: ~106 features â†’ Dimensionality reduction optional (PCA to 80)

---

## Model Architecture

### MLP Classifier (3 Hidden Layers)

```
Input Layer (106 features)
    â†“
Dense Layer 1 (128 units, ReLU)
â”œâ”€ Batch Normalization
â”œâ”€ Dropout (30%)
â””â”€ L2 Regularization (1e-4)
    â†“
Dense Layer 2 (64 units, ReLU)
â”œâ”€ Batch Normalization
â”œâ”€ Dropout (30%)
â””â”€ L2 Regularization (1e-4)
    â†“
Dense Layer 3 (32 units, ReLU)
â”œâ”€ Batch Normalization
â”œâ”€ Dropout (20%)
â””â”€ L2 Regularization (1e-4)
    â†“
Output Layer (3 units, Softmax)
    â†“
Predictions: [ASD, ADHD, Healthy]
```

**Total Parameters**: ~24,000 trainable parameters

*Based on: `mondtorsha/Speech-Emotion-Recognition/` (MLP pattern) and `x4nth055/emotion-recognition-using-speech/` (architecture)*

---

## Installation & Setup

### Prerequisites
- Python 3.8+
- CUDA 11.0+ (for GPU acceleration, optional)
- 4GB RAM minimum

### Step 1: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 2: Test Configuration

```python
from config.config import config
config.print_config()
```

This will display all 50+ configuration parameters.

---

## Usage

### 1. **Feature Extraction from Audio File**

```python
from src.feature_extraction.prosodic_extractor import ProsodicExtractor
from src.preprocessing.audio_preprocessor import AudioPreprocessor
from config.config import config

# Load and preprocess audio
preprocessor = AudioPreprocessor(config)
audio, sr = preprocessor.load_audio('path/to/audio.wav')
audio_clean = preprocessor.preprocess(audio, sr)

# Extract features
prosodic_extractor = ProsodicExtractor(config)
prosodic_features = prosodic_extractor.extract(audio_clean, sr)
```

### 2. **Training MLP Model**

```python
from src.models.mlp_classifier import MLPClassifier
from src.preprocessing.feature_normalizer import FeatureNormalizer
from config.config import config

# Initialize model
model = MLPClassifier(config)
model.build()

# Train with cross-validation
model.train_with_kfold(
    X_features,
    y_labels,
    n_splits=config.dataset.K_FOLDS
)
```

### 3. **Real-time Prediction**

```python
from src.models.mlp_classifier import MLPClassifier

# Load trained model
model = MLPClassifier(config)
model.load('models/saved/asd_adhd_mlp_v1.0.h5')

# Predict from microphone
prediction = model.predict_realtime(
    duration=5.0,  # seconds
    sample_rate=16000
)

print(f"Prediction: {prediction['class']}")
print(f"Confidence: {prediction['confidence']:.2%}")
```

### 4. **Web Dashboard (Streamlit)**

```bash
streamlit run streamlit_app/app.py
```

Open browser â†’ `http://localhost:8501`

---

## Reference Repositories & Adaptations

### Core Implementations

| Repository | Purpose | Adaptation |
|---|---|---|
| **x4nth055/emotion-recognition-using-speech** | MLP architecture, real-time recording | â†’ MLP classifier for ASD/ADHD |
| **mondtorsha/Speech-Emotion-Recognition** | MLP vs LSTM on RAVDESS | â†’ MLP architecture & training loop |
| **pyAudioAnalysis** | Audio feature extraction patterns | â†’ Spectral features extraction |
| **python_speech_features** | MFCC computation | â†’ MFCC extraction pipeline |
| **Parselmouth** | Prosodic analysis interface | â†’ Pitch, formants, jitter, shimmer |
| **Dinstein-Lab/ASDSpeech** | 49 autism acoustic features | â†’ Extended to 100+ features |
| **ronit1706/Autism-Detection** | Multi-class ML models (RF, SVM, ANN) | â†’ Multi-class (ASD/ADHD/Healthy) MLP |
| **56kd/MulitmodalDepressionDetection** | Hybrid CNN-LSTM, audio extraction | â†’ Feature extraction patterns |
| **MITESHPUTHRANNEU/Speech-Emotion-Analyzer** | Web dashboard with visualization | â†’ Streamlit app structure |

### Pattern Mappings

```
Emotion Recognition â†’ ASD/ADHD Detection
EmotionRecognizer   â†’ ASDDetector
emotion_label       â†’ diagnosis_label
8 emotion classes   â†’ 3 diagnostic classes (ASD/ADHD/Healthy)
RAVDESS dataset     â†’ Clinical voice recordings
```

---

## Training Pipeline

### 1. **Data Preparation**
- Load raw audio files (5-second samples)
- Split: 70% train, 15% val, 15% test
- Apply stratified K-fold CV (5 folds)

### 2. **Feature Extraction**
- Extract 106 acoustic + prosodic features per sample
- Normalize features (standardization)
- Optional: Apply PCA (reduce to 80 dimensions)

### 3. **Model Training**
- Initialize MLP with 128-64-32 architecture
- Apply batch norm + dropout regularization
- Train for max 100 epochs with early stopping
- Monitor validation accuracy (patience=15)

### 4. **Evaluation**
- Compute confusion matrix & metrics (accuracy, precision, recall, F1)
- Generate ROC curves per class
- Perform majority voting for frame-level aggregation
- Report class-wise performance

### 5. **Model Saving**
- Save model architecture (JSON)
- Save trained weights (H5)
- Save feature scaler & label encoder (pickle)
- Create model metadata (config, version, timestamp)

---

## Configuration Parameters

Key configuration in `config/config.py`:

| Parameter | Value | Purpose |
|---|---|---|
| `SAMPLE_RATE` | 16000 Hz | Audio sampling rate |
| `DURATION` | 5 sec | Voice sample length |
| `FRAME_LENGTH` | 400 samples | 25ms window |
| `HOP_LENGTH` | 160 samples | 10ms step (60% overlap) |
| `N_MFCC` | 13 | MFCC coefficients |
| `N_MEL` | 128 | Mel frequency bins |
| `F0_MIN` | 80 Hz | Minimum pitch frequency |
| `F0_MAX` | 400 Hz | Maximum pitch frequency |
| `HIDDEN_LAYERS` | [128, 64, 32] | MLP architecture |
| `DROPOUT_RATE` | 0.3 | Regularization |
| `LEARNING_RATE` | 0.001 | Adam optimizer LR |
| `EPOCHS` | 100 | Max training epochs |
| `BATCH_SIZE` | 32 | Training batch size |
| `K_FOLDS` | 5 | Cross-validation folds |

---

## Performance Targets

Based on reference implementations:

- **Accuracy**: â‰¥ 85% (binary classification baseline)
- **Precision**: â‰¥ 0.83 per class
- **Recall**: â‰¥ 0.82 per class
- **F1 Score**: â‰¥ 0.82

*Reference: ronit1706/Autism-Detection achieved 90% on binary ASD/non-ASD*

---

## API Endpoints (Optional)

```
POST /predict
â”œâ”€ Input: Audio file (WAV, MP3, OGG)
â””â”€ Output: {class, confidence, features, markers}

POST /predict-realtime
â”œâ”€ Input: Audio stream
â””â”€ Output: Real-time predictions

GET /model/info
â””â”€ Output: Model architecture, version, performance metrics
```

---

## Troubleshooting

### Issue: Low accuracy
- **Solution**: Check feature normalization, verify class balance, increase training epochs

### Issue: Memory error
- **Solution**: Reduce batch size, use data generators, enable gradient checkpointing

### Issue: Slow feature extraction
- **Solution**: Enable GPU acceleration, use vectorized operations (NumPy/SciPy)

### Issue: Noisy predictions
- **Solution**: Increase confidence threshold, use ensemble predictions

---

## Future Enhancements

- [ ] **Transfer Learning**: Pre-trained models (Speech2Vec, WavLM)
- [ ] **Ensemble Methods**: Combine MLP with Random Forest, SVM
- [ ] **Attention Mechanisms**: Self-attention over time dimension
- [ ] **Temporal Modeling**: LSTM/GRU layers for sequence modeling
- [ ] **Multimodal**: Combine voice with text analysis (BERT)
- [ ] **Explainability**: SHAP/LIME for feature importance
- [ ] **Mobile Deployment**: TensorFlow Lite for mobile apps
- [ ] **A/B Testing**: Clinical validation framework

---

## Citation & References

This project adapts implementations and patterns from multiple open-source repositories:

```bibtex
@misc{emotion_recognition_speech,
  title={Speech Emotion Recognition using MLP},
  author={x4nth055},
  url={https://github.com/x4nth055/emotion-recognition-using-speech}
}

@misc{asd_speech,
  title={Autism Spectrum Disorder Detection from Speech},
  author={Dinstein Lab},
  url={https://github.com/Dinstein-Lab/ASDSpeech}
}

@misc{autism_detection,
  title={Autism Detection using Machine Learning},
  author={Ronit Khurana},
  url={https://github.com/ronit1706/Autism-Detection}
}
```

---

## License

MIT License - See LICENSE file for details

---

## Contact & Support

For questions or issues:
- Create an issue on GitHub
- Check documentation in `/notebooks/`
- Review configuration in `/config/config.py`

---

**Last Updated**: November 2025  
**Version**: 1.0.0  
**Status**: Active Development ðŸš€
