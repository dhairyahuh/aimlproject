{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1980e86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7f9d062",
   "metadata": {},
   "source": [
    "# ASD/ADHD Voice Detection - Phase 1: Feature Extraction Tutorial\n",
    "\n",
    "This notebook walks you through **step-by-step feature extraction** from raw audio files. \n",
    "\n",
    "**Key Goals:**\n",
    "- Understand how audio features are computed\n",
    "- Visualize what each feature type captures\n",
    "- Learn MFCC, Spectral, and Prosodic features\n",
    "- See the 106-feature vector that feeds the MLP model\n",
    "- Identify features for refinement\n",
    "\n",
    "**What You'll Learn:**\n",
    "1. How audio preprocessing standardizes samples\n",
    "2. How MFCC features capture speech dynamics\n",
    "3. How spectral features describe frequency content\n",
    "4. How prosodic features reveal voice patterns\n",
    "5. How features combine into a single 106-dimensional vector\n",
    "\n",
    "**Let's get started!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d731c81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install librosa\n",
    "%pip install soundfile\n",
    "!pip install numpy\n",
    "# Add parent directory to path so we can import our modules\n",
    "sys.path.insert(0, os.path.abspath('../..'))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "# Import our custom modules\n",
    "from ASD_ADHD_Detection.config.config import config\n",
    "from ASD_ADHD_Detection.src.preprocessing.audio_preprocessor import AudioPreprocessor\n",
    "from ASD_ADHD_Detection.src.feature_extraction.mfcc_extractor import MFCCExtractor\n",
    "from ASD_ADHD_Detection.src.feature_extraction.spectral_extractor import SpectralExtractor\n",
    "from ASD_ADHD_Detection.src.feature_extraction.prosodic_extractor import ProsodicExtractor\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"üìä Numpy version: {np.__version__}\")\n",
    "print(f\"üéµ Librosa version: {librosa.__version__}\")\n",
    "print(f\"‚öôÔ∏è  Config loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a93c7f",
   "metadata": {},
   "source": [
    "## Section 1: Create Synthetic Audio Sample\n",
    "\n",
    "Since we don't have real audio files yet, we'll create a synthetic voice sample that contains typical speech characteristics. This lets us understand the pipeline before working with real data.\n",
    "\n",
    "**What we're creating:**\n",
    "- A synthetic voice sample at 16 kHz (standard speech rate)\n",
    "- 5 seconds of audio\n",
    "- Contains pitch variations and frequency content\n",
    "- Will be processed through our full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f273d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic voice sample\n",
    "sr = config.audio.SAMPLE_RATE  # 16000 Hz\n",
    "duration = config.audio.DURATION  # 5 seconds\n",
    "t = np.linspace(0, duration, int(sr * duration), False)\n",
    "\n",
    "# Create a voice-like signal with multiple components\n",
    "# - Base pitch around 120 Hz (male voice)\n",
    "# - Pitch variations (natural prosody)\n",
    "# - Formants (voice characteristics)\n",
    "# - Some noise (realistic voice)\n",
    "\n",
    "fundamental_freq = 120  # Hz\n",
    "pitch_variation = 20 * np.sin(2 * np.pi * 1.5 * t)  # 1.5 Hz vibrato\n",
    "f0 = fundamental_freq + pitch_variation\n",
    "\n",
    "# Create harmonic content (fundamental + harmonics)\n",
    "audio = np.zeros_like(t)\n",
    "for harmonic in range(1, 8):  # 7 harmonics\n",
    "    amplitude = 1.0 / harmonic  # Amplitude decreases with harmonics\n",
    "    audio += amplitude * np.sin(2 * np.pi * f0 * harmonic * t)\n",
    "\n",
    "# Add spectral envelope (formants)\n",
    "# Formant frequencies (typical for vowel sounds)\n",
    "formants = [200, 1200, 2300]  # Hz\n",
    "for formant_freq in formants:\n",
    "    bandwidth = 50  # Hz\n",
    "    envelope = np.exp(-np.pi * (np.arange(len(t)) / sr - 2.5) ** 2 / 0.5)\n",
    "    audio += 0.1 * np.sin(2 * np.pi * formant_freq * t) * envelope\n",
    "\n",
    "# Add realistic noise\n",
    "noise = np.random.normal(0, 0.02, len(audio))\n",
    "audio = audio + noise\n",
    "\n",
    "# Normalize\n",
    "audio = audio / np.max(np.abs(audio)) * 0.9\n",
    "\n",
    "print(f\"‚úÖ Synthetic audio created!\")\n",
    "print(f\"   Duration: {duration} seconds\")\n",
    "print(f\"   Sample rate: {sr} Hz\")\n",
    "print(f\"   Total samples: {len(audio)}\")\n",
    "print(f\"   Audio range: [{audio.min():.3f}, {audio.max():.3f}]\")\n",
    "print(f\"   RMS energy: {np.sqrt(np.mean(audio**2)):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddc12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the synthetic audio\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 8))\n",
    "\n",
    "# Time domain waveform\n",
    "axes[0].plot(t, audio, linewidth=0.5, alpha=0.8)\n",
    "axes[0].set_title('Raw Audio Waveform (Time Domain)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Time (s)')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Spectrogram\n",
    "D = librosa.stft(audio)\n",
    "S_db = librosa.power_to_db(np.abs(D)**2, ref=np.max)\n",
    "img = librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='log', ax=axes[1], cmap='viridis')\n",
    "axes[1].set_title('Spectrogram (Frequency Content Over Time)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency (Hz)')\n",
    "plt.colorbar(img, ax=axes[1], format='%+2.0f dB')\n",
    "\n",
    "# Power spectrum (average across time)\n",
    "freqs = np.fft.rfftfreq(len(audio), 1/sr)\n",
    "spectrum = np.abs(np.fft.rfft(audio))**2\n",
    "axes[2].semilogy(freqs[:5000], spectrum[:5000], linewidth=1, alpha=0.8)\n",
    "axes[2].set_title('Power Spectrum (Average Frequency Content)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Frequency (Hz)')\n",
    "axes[2].set_ylabel('Power')\n",
    "axes[2].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Audio visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4a5624",
   "metadata": {},
   "source": [
    "## Section 2: Audio Preprocessing\n",
    "\n",
    "Before we extract features, we must standardize the audio:\n",
    "1. **Trim Silence** - Remove quiet sections at beginning/end\n",
    "2. **Normalize** - Scale amplitude to [-1, 1] range\n",
    "3. **Pad/Truncate** - Ensure exactly 5 seconds\n",
    "\n",
    "This ensures all audio inputs are consistent shape and quality.\n",
    "\n",
    "**Why this matters for ASD/ADHD detection:**\n",
    "- Silence trimming focuses on actual speech content\n",
    "- Normalization prevents volume variation from affecting features\n",
    "- Fixed length ensures batch processing compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0925026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Trim Silence\n",
    "preprocessor = AudioPreprocessor(config)\n",
    "\n",
    "# Before preprocessing\n",
    "print(\"BEFORE PREPROCESSING:\")\n",
    "print(f\"   Duration: {len(audio) / sr:.2f} seconds\")\n",
    "print(f\"   Sample count: {len(audio)}\")\n",
    "print(f\"   Amplitude range: [{audio.min():.4f}, {audio.max():.4f}]\")\n",
    "print(f\"   RMS energy: {np.sqrt(np.mean(audio**2)):.4f}\")\n",
    "\n",
    "# Apply preprocessing\n",
    "audio_trimmed = librosa.effects.trim(audio, top_db=config.audio.TRIM_THRESHOLD_DB)[0]\n",
    "audio_normalized = audio_trimmed / np.max(np.abs(audio_trimmed))\n",
    "target_samples = int(sr * config.audio.DURATION)\n",
    "if len(audio_normalized) < target_samples:\n",
    "    audio_processed = np.pad(audio_normalized, (0, target_samples - len(audio_normalized)), mode='constant')\n",
    "elif len(audio_normalized) > target_samples:\n",
    "    audio_processed = audio_normalized[:target_samples]\n",
    "else:\n",
    "    audio_processed = audio_normalized\n",
    "\n",
    "print(\"\\nAFTER PREPROCESSING:\")\n",
    "print(f\"   Duration: {len(audio_processed) / sr:.2f} seconds\")\n",
    "print(f\"   Sample count: {len(audio_processed)}\")\n",
    "print(f\"   Amplitude range: [{audio_processed.min():.4f}, {audio_processed.max():.4f}]\")\n",
    "print(f\"   RMS energy: {np.sqrt(np.mean(audio_processed**2)):.4f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Audio preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0f944",
   "metadata": {},
   "source": [
    "## Section 3: MFCC Feature Extraction (52 Features)\n",
    "\n",
    "**What are MFCCs?**\n",
    "Mel-Frequency Cepstral Coefficients (MFCCs) are the most popular speech features. They:\n",
    "- Mimic human hearing by scaling frequencies to mel-scale (logarithmic)\n",
    "- Capture spectral characteristics that distinguish phonemes\n",
    "- Include velocity (delta) and acceleration (delta-delta) for dynamics\n",
    "\n",
    "**Our configuration:**\n",
    "- **Base MFCCs:** 13 coefficients (0-12)\n",
    "- **Delta (Œî):** 13 first-order derivatives (change over time)\n",
    "- **Delta-Delta (ŒîŒî):** 13 second-order derivatives (acceleration)\n",
    "- **Statistics:** mean, std, min, max per feature type\n",
    "- **Total:** 13 √ó 4 = 52 features\n",
    "\n",
    "**Why MFCC for ASD/ADHD detection:**\n",
    "- MFCCs capture voice quality changes in autism spectrum disorders\n",
    "- Deltas reveal speech rate variations\n",
    "- Delta-deltas show prosodic changes and speech fluency issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50378e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MFCC features\n",
    "mfcc_extractor = MFCCExtractor(config)\n",
    "\n",
    "# Step 1: Extract base MFCCs\n",
    "print(\"üéµ EXTRACTING MFCC FEATURES...\")\n",
    "mfcc = mfcc_extractor.extract_mfcc(audio_processed, sr)\n",
    "print(f\"   Base MFCCs shape: {mfcc.shape}\")  # Should be (13, time_steps)\n",
    "print(f\"   Time steps: {mfcc.shape[1]}\")\n",
    "\n",
    "# Step 2: Extract delta (velocity)\n",
    "delta = mfcc_extractor.extract_delta(mfcc, order=1)\n",
    "print(f\"   Delta (Œî) shape: {delta.shape}\")  # Should be (13, time_steps)\n",
    "\n",
    "# Step 3: Extract delta-delta (acceleration)\n",
    "delta_delta = mfcc_extractor.extract_delta(mfcc, order=2)\n",
    "print(f\"   Delta-Delta (ŒîŒî) shape: {delta_delta.shape}\")  # Should be (13, time_steps)\n",
    "\n",
    "# Step 4: Aggregate to 52 features\n",
    "mfcc_features = mfcc_extractor.extract(audio_processed, sr)\n",
    "mfcc_names = mfcc_extractor.get_feature_names()\n",
    "\n",
    "print(f\"\\n‚úÖ MFCC extraction complete!\")\n",
    "print(f\"   Total MFCC features: {len(mfcc_features)}\")\n",
    "print(f\"   Features shape: {mfcc_features.shape}\")\n",
    "print(f\"   Feature vector (first 10 values):\\n   {mfcc_features[:10]}\")\n",
    "print(f\"\\n   Feature names (first 10):\")\n",
    "for i, name in enumerate(mfcc_names[:10]):\n",
    "    print(f\"     [{i:2d}] {name:30s} = {mfcc_features[i]:8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b81b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MFCC components\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot base MFCCs\n",
    "im1 = axes[0].imshow(mfcc, aspect='auto', origin='lower', cmap='viridis', interpolation='nearest')\n",
    "axes[0].set_title('Base MFCCs (13 Coefficients √ó Time)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Time Frame')\n",
    "axes[0].set_ylabel('MFCC Coefficient')\n",
    "axes[0].set_yticks(range(13))\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "# Plot delta\n",
    "im2 = axes[1].imshow(delta, aspect='auto', origin='lower', cmap='viridis', interpolation='nearest')\n",
    "axes[1].set_title('Delta - MFCC Velocity (Rate of Change)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Time Frame')\n",
    "axes[1].set_ylabel('MFCC Coefficient')\n",
    "axes[1].set_yticks(range(13))\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "\n",
    "# Plot delta-delta\n",
    "im3 = axes[2].imshow(delta_delta, aspect='auto', origin='lower', cmap='viridis', interpolation='nearest')\n",
    "axes[2].set_title('Delta-Delta - MFCC Acceleration (Rate of Change of Rate)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Time Frame')\n",
    "axes[2].set_ylabel('MFCC Coefficient')\n",
    "axes[2].set_yticks(range(13))\n",
    "plt.colorbar(im3, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä MFCC visualization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MFCC feature statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "fig.suptitle('MFCC Feature Statistics (52 Features)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Extract statistics groups\n",
    "base_stats = mfcc_features[:13]\n",
    "delta_stats = mfcc_features[13:26]\n",
    "delta_delta_stats = mfcc_features[26:39]\n",
    "other_stats = mfcc_features[39:52]\n",
    "\n",
    "# Plot 1: Base MFCCs\n",
    "axes[0, 0].bar(range(13), base_stats, alpha=0.7, color='steelblue')\n",
    "axes[0, 0].set_title('Base MFCC Coefficients (Mean)', fontsize=11)\n",
    "axes[0, 0].set_xlabel('Coefficient Index')\n",
    "axes[0, 0].set_ylabel('Mean Value')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Delta\n",
    "axes[0, 1].bar(range(13), delta_stats, alpha=0.7, color='coral')\n",
    "axes[0, 1].set_title('Delta - Velocity (Mean)', fontsize=11)\n",
    "axes[0, 1].set_xlabel('Coefficient Index')\n",
    "axes[0, 1].set_ylabel('Mean Value')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Delta-Delta\n",
    "axes[1, 0].bar(range(13), delta_delta_stats, alpha=0.7, color='lightgreen')\n",
    "axes[1, 0].set_title('Delta-Delta - Acceleration (Mean)', fontsize=11)\n",
    "axes[1, 0].set_xlabel('Coefficient Index')\n",
    "axes[1, 0].set_ylabel('Mean Value')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Additional statistics\n",
    "stat_labels = ['Std', 'Min', 'Max'] + ['Reserved'] * 10\n",
    "axes[1, 1].bar(range(len(other_stats)), other_stats, alpha=0.7, color='mediumpurple')\n",
    "axes[1, 1].set_title('Additional Statistics', fontsize=11)\n",
    "axes[1, 1].set_xlabel('Statistic Index')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ MFCC statistics visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4921d5",
   "metadata": {},
   "source": [
    "## Section 4: Spectral Feature Extraction (24 Features)\n",
    "\n",
    "**What are Spectral Features?**\n",
    "Spectral features describe the frequency content of speech:\n",
    "- **Centroid:** Where is the \"center of mass\" of the spectrum?\n",
    "- **Rolloff:** At what frequency do 95% of energy concentrate?\n",
    "- **Bandwidth:** How spread out is the energy?\n",
    "- **Zero-Crossing Rate (ZCR):** How many times does the signal cross zero?\n",
    "- **Energy:** How much power in the signal?\n",
    "- **Chroma:** Distribution across 12 pitch classes\n",
    "\n",
    "**Our configuration:**\n",
    "- **6 base spectral features** (centroid, rolloff, bandwidth, ZCR, RMS energy, log-energy)\n",
    "- **12 chroma features** (pitch class distribution)\n",
    "- **Statistics:** mean, std, min, max per feature\n",
    "- **Total:** (6 + 12) √ó 4 / variations = 24 features (simplified version)\n",
    "\n",
    "**Why for ASD/ADHD detection:**\n",
    "- Spectral centroid changes with voice tension and emotional state\n",
    "- Energy variations reveal speech rate and breathing patterns\n",
    "- Chroma features reflect pitch variations associated with prosodic disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2cdc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract spectral features\n",
    "spectral_extractor = SpectralExtractor(config)\n",
    "\n",
    "print(\"üéµ EXTRACTING SPECTRAL FEATURES...\")\n",
    "\n",
    "# Extract individual spectral components\n",
    "spectral_centroid = spectral_extractor.extract_spectral_centroid(audio_processed)\n",
    "spectral_rolloff = spectral_extractor.extract_spectral_rolloff(audio_processed)\n",
    "spectral_bandwidth = spectral_extractor.extract_spectral_bandwidth(audio_processed)\n",
    "zcr = spectral_extractor.extract_zcr(audio_processed)\n",
    "rms_energy = spectral_extractor.extract_rms_energy(audio_processed)\n",
    "chroma = spectral_extractor.extract_chroma(audio_processed)\n",
    "\n",
    "print(f\"   Spectral Centroid shape: {spectral_centroid.shape}\")\n",
    "print(f\"   Spectral Rolloff shape: {spectral_rolloff.shape}\")\n",
    "print(f\"   Spectral Bandwidth shape: {spectral_bandwidth.shape}\")\n",
    "print(f\"   Zero-Crossing Rate shape: {zcr.shape}\")\n",
    "print(f\"   RMS Energy shape: {rms_energy.shape}\")\n",
    "print(f\"   Chroma shape: {chroma.shape}\")\n",
    "\n",
    "# Extract complete spectral features\n",
    "spectral_features = spectral_extractor.extract(audio_processed, sr)\n",
    "spectral_names = spectral_extractor.get_feature_names()\n",
    "\n",
    "print(f\"\\n‚úÖ Spectral extraction complete!\")\n",
    "print(f\"   Total spectral features: {len(spectral_features)}\")\n",
    "print(f\"   Features shape: {spectral_features.shape}\")\n",
    "print(f\"   Feature vector (first 10 values):\\n   {spectral_features[:10]}\")\n",
    "print(f\"\\n   Feature names (first 10):\")\n",
    "for i, name in enumerate(spectral_names[:10]):\n",
    "    print(f\"     [{i:2d}] {name:30s} = {spectral_features[i]:8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8781f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spectral components\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "fig.suptitle('Spectral Features Over Time', fontsize=14, fontweight='bold')\n",
    "\n",
    "time_axis = np.linspace(0, config.audio.DURATION, len(spectral_centroid))\n",
    "\n",
    "# Plot 1: Spectral Centroid\n",
    "axes[0, 0].plot(time_axis, spectral_centroid, linewidth=1.5, color='steelblue')\n",
    "axes[0, 0].set_title('Spectral Centroid (Hz)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Frequency (Hz)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].fill_between(time_axis, spectral_centroid, alpha=0.3)\n",
    "\n",
    "# Plot 2: Spectral Rolloff\n",
    "axes[0, 1].plot(time_axis, spectral_rolloff, linewidth=1.5, color='coral')\n",
    "axes[0, 1].set_title('Spectral Rolloff (Hz)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Frequency (Hz)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].fill_between(time_axis, spectral_rolloff, alpha=0.3, color='coral')\n",
    "\n",
    "# Plot 3: Spectral Bandwidth\n",
    "axes[0, 2].plot(time_axis, spectral_bandwidth, linewidth=1.5, color='lightgreen')\n",
    "axes[0, 2].set_title('Spectral Bandwidth (Hz)', fontsize=11, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Time (s)')\n",
    "axes[0, 2].set_ylabel('Bandwidth (Hz)')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[0, 2].fill_between(time_axis, spectral_bandwidth, alpha=0.3, color='lightgreen')\n",
    "\n",
    "# Plot 4: Zero-Crossing Rate\n",
    "axes[1, 0].plot(time_axis, zcr, linewidth=1.5, color='mediumpurple')\n",
    "axes[1, 0].set_title('Zero-Crossing Rate', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Time (s)')\n",
    "axes[1, 0].set_ylabel('ZCR')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].fill_between(time_axis, zcr, alpha=0.3, color='mediumpurple')\n",
    "\n",
    "# Plot 5: RMS Energy\n",
    "axes[1, 1].plot(time_axis, rms_energy, linewidth=1.5, color='goldenrod')\n",
    "axes[1, 1].set_title('RMS Energy', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Time (s)')\n",
    "axes[1, 1].set_ylabel('Energy')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].fill_between(time_axis, rms_energy, alpha=0.3, color='goldenrod')\n",
    "\n",
    "# Plot 6: Chroma features\n",
    "im = axes[1, 2].imshow(chroma, aspect='auto', origin='lower', cmap='viridis', interpolation='nearest')\n",
    "axes[1, 2].set_title('Chromagram (12 Pitch Classes)', fontsize=11, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Time Frame')\n",
    "axes[1, 2].set_ylabel('Pitch Class')\n",
    "axes[1, 2].set_yticks(range(12))\n",
    "axes[1, 2].set_yticklabels(['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'])\n",
    "plt.colorbar(im, ax=axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Spectral features visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ef1939",
   "metadata": {},
   "source": [
    "## Section 5: Prosodic Feature Extraction (19+ Features)\n",
    "\n",
    "**What are Prosodic Features?**\n",
    "Prosody is the \"music\" of speech - pitch, timing, and emphasis patterns. These features are **especially important for ASD/ADHD detection** because:\n",
    "\n",
    "- **Fundamental Frequency (F0):** The pitch of the voice\n",
    "- **Formants:** Resonances of the vocal tract (F1, F2, F3)\n",
    "- **Jitter:** Pitch instability (sign of voice disorders, **AUTISM MARKER**)\n",
    "- **Shimmer:** Amplitude instability (sign of vocal fatigue, **AUTISM MARKER**)\n",
    "- **Harmonic-to-Noise Ratio (HNR):** Voice quality measure\n",
    "\n",
    "**Our configuration:**\n",
    "- **F0 statistics:** 8 features (mean, std, min, max, median, range, coefficient of variation, voiced rate)\n",
    "- **Formants:** 6 features (F1, F2, F3 mean magnitudes)\n",
    "- **Jitter:** 1 feature (pitch perturbation) - **AUTISM MARKER**\n",
    "- **Shimmer:** 1 feature (amplitude perturbation) - **AUTISM MARKER**\n",
    "- **HNR:** 1 feature (harmonic-to-noise ratio)\n",
    "- **Voice Quality:** 2 features (voice activity rate, voice breaks)\n",
    "- **Total:** 19 features\n",
    "\n",
    "**Why Prosodic for ASD/ADHD:**\n",
    "- **Autism:** Associated with increased jitter/shimmer, monotone pitch, reduced F0 variation\n",
    "- **ADHD:** Associated with faster speech rate, irregular timing, energy variability\n",
    "- **Healthy:** Typical pitch variation (150-250 Hz for males), stable voice quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe9f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract prosodic features\n",
    "prosodic_extractor = ProsodicExtractor(config)\n",
    "\n",
    "print(\"üéµ EXTRACTING PROSODIC FEATURES...\")\n",
    "\n",
    "# Extract F0 (fundamental frequency)\n",
    "f0_result = prosodic_extractor.extract_f0_librosa(audio_processed, sr)\n",
    "if f0_result is not None:\n",
    "    f0 = f0_result\n",
    "    print(f\"   F0 extraction successful: {f0.shape}\")\n",
    "else:\n",
    "    # Create placeholder F0 if extraction fails\n",
    "    f0 = np.linspace(100, 150, len(audio_processed) // 512)\n",
    "    print(f\"   Using synthetic F0: {f0.shape}\")\n",
    "\n",
    "# Extract complete prosodic features\n",
    "try:\n",
    "    prosodic_features = prosodic_extractor.extract(audio_processed, sr)\n",
    "    prosodic_names = prosodic_extractor.get_feature_names()\n",
    "    print(f\"\\n‚úÖ Prosodic extraction complete!\")\n",
    "    print(f\"   Total prosodic features: {len(prosodic_features)}\")\n",
    "    print(f\"   Features shape: {prosodic_features.shape}\")\n",
    "    print(f\"   Feature vector (all values):\\n   {prosodic_features}\")\n",
    "    print(f\"\\n   All feature names:\")\n",
    "    for i, name in enumerate(prosodic_names):\n",
    "        print(f\"     [{i:2d}] {name:30s} = {prosodic_features[i]:8.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error in prosodic extraction: {e}\")\n",
    "    # Create synthetic prosodic features for demonstration\n",
    "    prosodic_features = np.random.randn(19) * 10 + 50\n",
    "    prosodic_names = [f'Prosodic_feature_{i}' for i in range(19)]\n",
    "    print(f\"   Using synthetic prosodic features for demonstration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prosodic features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Prosodic Features Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 1: F0 contour over time\n",
    "time_f0 = np.linspace(0, config.audio.DURATION, len(f0))\n",
    "axes[0, 0].plot(time_f0, f0, linewidth=2, color='steelblue', marker='o', markersize=3)\n",
    "axes[0, 0].fill_between(time_f0, f0, alpha=0.3, color='steelblue')\n",
    "axes[0, 0].set_title('F0 Contour (Fundamental Frequency Over Time)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Frequency (Hz)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylim([50, 300])\n",
    "\n",
    "# Plot 2: Prosodic feature breakdown\n",
    "feature_groups = {\n",
    "    'F0 Stats': prosodic_features[0:8] if len(prosodic_features) >= 8 else prosodic_features[0:3],\n",
    "    'Formants': prosodic_features[8:11] if len(prosodic_features) >= 11 else [prosodic_features[3]],\n",
    "    'Voice Quality': prosodic_features[11:19] if len(prosodic_features) >= 19 else prosodic_features[4:]\n",
    "}\n",
    "\n",
    "colors_dict = {'F0 Stats': 'steelblue', 'Formants': 'coral', 'Voice Quality': 'lightgreen'}\n",
    "positions = []\n",
    "labels = []\n",
    "values = []\n",
    "colors = []\n",
    "pos = 0\n",
    "\n",
    "for group_name, group_values in feature_groups.items():\n",
    "    for val in group_values:\n",
    "        positions.append(pos)\n",
    "        labels.append(group_name)\n",
    "        values.append(val)\n",
    "        colors.append(colors_dict[group_name])\n",
    "        pos += 1\n",
    "\n",
    "axes[0, 1].bar(positions, values, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "axes[0, 1].set_title('Prosodic Features Values', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[0, 1].set_xticks(range(len(values)))\n",
    "axes[0, 1].set_xticklabels(range(len(values)), fontsize=8)\n",
    "\n",
    "# Plot 3: Feature statistics\n",
    "feature_stats = {\n",
    "    'Mean': np.mean(prosodic_features),\n",
    "    'Std': np.std(prosodic_features),\n",
    "    'Min': np.min(prosodic_features),\n",
    "    'Max': np.max(prosodic_features),\n",
    "    'Median': np.median(prosodic_features)\n",
    "}\n",
    "\n",
    "axes[1, 0].bar(feature_stats.keys(), feature_stats.values(), color='mediumpurple', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Prosodic Features Statistics', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Feature importance indicators\n",
    "axes[1, 1].text(0.1, 0.9, 'Key ASD/ADHD Markers:', fontsize=12, fontweight='bold', transform=axes[1, 1].transAxes)\n",
    "markers_text = \"\"\"\n",
    "üî¥ Jitter: Pitch instability\n",
    "   (‚Üë in Autism Spectrum Disorders)\n",
    "\n",
    "üî¥ Shimmer: Amplitude instability  \n",
    "   (‚Üë in Autism Spectrum Disorders)\n",
    "\n",
    "üü° F0 Variation: Pitch range\n",
    "   (‚Üì monotone in Autism)\n",
    "\n",
    "üü° Speech Rate: From timing\n",
    "   (‚Üë irregular in ADHD)\n",
    "\n",
    "üü¢ Voice Quality: HNR\n",
    "   (‚Üì noisy in voice disorders)\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.45, markers_text, fontsize=10, family='monospace', transform=axes[1, 1].transAxes,\n",
    "                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Prosodic features visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d1420",
   "metadata": {},
   "source": [
    "## Section 6: Aggregate All Features (106 Total)\n",
    "\n",
    "Now we combine all three feature types into a single **106-dimensional feature vector**:\n",
    "\n",
    "| Feature Type | Count | Description |\n",
    "|---|---|---|\n",
    "| **MFCC** | 52 | Speech spectral characteristics (base + velocity + acceleration) |\n",
    "| **Spectral** | 24 | Frequency content & energy distribution |\n",
    "| **Prosodic** | 19+ | Pitch, voice quality, and timing patterns |\n",
    "| **TOTAL** | **106** | Complete acoustic profile |\n",
    "\n",
    "This 106-dimensional vector is the **input to our MLP neural network classifier**.\n",
    "\n",
    "**Key insights:**\n",
    "- Each feature type captures different aspects of voice\n",
    "- Together they provide comprehensive acoustic profile\n",
    "- The MLP learns which combinations are diagnostic for ASD/ADHD\n",
    "- You can disable specific feature types to test their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840741d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features into single 106-dimensional vector\n",
    "print(\"üéµ AGGREGATING ALL FEATURES...\\n\")\n",
    "\n",
    "# Create the complete feature vector\n",
    "complete_features = np.concatenate([mfcc_features, spectral_features, prosodic_features])\n",
    "complete_names = mfcc_names + spectral_names + prosodic_names\n",
    "\n",
    "print(f\"   MFCC features:      {len(mfcc_features):3d} dimensions\")\n",
    "print(f\"   Spectral features:  {len(spectral_features):3d} dimensions\")\n",
    "print(f\"   Prosodic features:  {len(prosodic_features):3d} dimensions\")\n",
    "print(f\"   \" + \"=\" * 40)\n",
    "print(f\"   TOTAL FEATURES:     {len(complete_features):3d} dimensions\")\n",
    "\n",
    "print(f\"\\n‚úÖ Feature aggregation complete!\")\n",
    "print(f\"   Complete feature vector shape: {complete_features.shape}\")\n",
    "print(f\"   Total feature names: {len(complete_names)}\")\n",
    "print(f\"\\n   Feature vector (complete):\")\n",
    "for i in range(0, len(complete_features), 10):\n",
    "    end_idx = min(i + 10, len(complete_features))\n",
    "    print(f\"   [{i:3d}-{end_idx-1:3d}]: {complete_features[i:end_idx]}\")\n",
    "\n",
    "# Statistics on the complete feature vector\n",
    "print(f\"\\nüìä Feature Vector Statistics:\")\n",
    "print(f\"   Mean:   {np.mean(complete_features):8.4f}\")\n",
    "print(f\"   Std:    {np.std(complete_features):8.4f}\")\n",
    "print(f\"   Min:    {np.min(complete_features):8.4f}\")\n",
    "print(f\"   Max:    {np.max(complete_features):8.4f}\")\n",
    "print(f\"   Median: {np.median(complete_features):8.4f}\")\n",
    "print(f\"   Q25:    {np.percentile(complete_features, 25):8.4f}\")\n",
    "print(f\"   Q75:    {np.percentile(complete_features, 75):8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cef55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize complete feature vector\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = GridSpec(3, 2, figure=fig)\n",
    "\n",
    "fig.suptitle('Complete 106-Dimensional Feature Vector', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 1: All features as bar chart\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "colors_feat = (['steelblue'] * len(mfcc_features) + \n",
    "               ['coral'] * len(spectral_features) + \n",
    "               ['lightgreen'] * len(prosodic_features))\n",
    "bars = ax1.bar(range(len(complete_features)), complete_features, color=colors_feat, alpha=0.7, edgecolor='black', linewidth=0.5)\n",
    "ax1.set_title('All 106 Features (MFCC | Spectral | Prosodic)', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Feature Index')\n",
    "ax1.set_ylabel('Feature Value')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.axvline(x=len(mfcc_features), color='red', linestyle='--', linewidth=2, alpha=0.5, label='MFCC|Spectral')\n",
    "ax1.axvline(x=len(mfcc_features) + len(spectral_features), color='purple', linestyle='--', linewidth=2, alpha=0.5, label='Spectral|Prosodic')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Feature distribution\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.hist(complete_features, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "ax2.axvline(x=np.mean(complete_features), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(complete_features):.2f}')\n",
    "ax2.set_title('Feature Distribution', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Feature Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Feature type breakdown\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "feature_types = ['MFCC\\n(52)', 'Spectral\\n(24)', 'Prosodic\\n(19)']\n",
    "feature_counts = [len(mfcc_features), len(spectral_features), len(prosodic_features)]\n",
    "colors_pie = ['steelblue', 'coral', 'lightgreen']\n",
    "wedges, texts, autotexts = ax3.pie(feature_counts, labels=feature_types, autopct='%1.1f%%', \n",
    "                                     colors=colors_pie, startangle=90, textprops={'fontsize': 11, 'weight': 'bold'})\n",
    "ax3.set_title('Feature Type Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 4: Cumulative contribution\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "sorted_features = np.sort(np.abs(complete_features))[::-1]\n",
    "cumsum = np.cumsum(sorted_features) / np.sum(np.abs(complete_features))\n",
    "ax4.plot(range(len(cumsum)), cumsum, linewidth=2, color='steelblue', marker='o', markersize=4)\n",
    "ax4.axhline(y=0.8, color='red', linestyle='--', linewidth=2, alpha=0.5, label='80% threshold')\n",
    "ax4.axhline(y=0.95, color='orange', linestyle='--', linewidth=2, alpha=0.5, label='95% threshold')\n",
    "ax4.fill_between(range(len(cumsum)), cumsum, alpha=0.3, color='steelblue')\n",
    "ax4.set_title('Cumulative Feature Contribution (Sorted by Absolute Value)', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Feature Index (sorted)')\n",
    "ax4.set_ylabel('Cumulative Contribution')\n",
    "ax4.set_ylim([0, 1.05])\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Feature vector visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4742a1",
   "metadata": {},
   "source": [
    "## Section 7: Feature Importance & Correlation\n",
    "\n",
    "Now let's understand which features are most variable and how they correlate. This helps identify:\n",
    "- Which features have meaningful variation\n",
    "- Which features are redundant (highly correlated)\n",
    "- Which features might be most diagnostic\n",
    "\n",
    "**Interpretation Guide:**\n",
    "- **High variance features:** More discriminative potential\n",
    "- **Correlated features:** May be redundant (could use dimensionality reduction)\n",
    "- **Low correlation features:** Capture different aspects of voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63773cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature importance DataFrame for analysis\n",
    "feature_df = pd.DataFrame({\n",
    "    'Feature': complete_names,\n",
    "    'Value': complete_features,\n",
    "    'Abs_Value': np.abs(complete_features),\n",
    "    'Type': (['MFCC'] * len(mfcc_features) + \n",
    "             ['Spectral'] * len(spectral_features) + \n",
    "             ['Prosodic'] * len(prosodic_features))\n",
    "})\n",
    "\n",
    "# Sort by absolute value\n",
    "feature_df_sorted = feature_df.sort_values('Abs_Value', ascending=False)\n",
    "\n",
    "print(\"üìä TOP 20 MOST IMPORTANT FEATURES (by magnitude):\\n\")\n",
    "print(feature_df_sorted.head(20).to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nüìä FEATURE STATISTICS BY TYPE:\\n\")\n",
    "print(feature_df.groupby('Type')['Value'].describe())\n",
    "\n",
    "# Feature variance by type\n",
    "print(\"\\nüìä FEATURE VARIANCE BY TYPE:\\n\")\n",
    "for ftype in ['MFCC', 'Spectral', 'Prosodic']:\n",
    "    vals = feature_df[feature_df['Type'] == ftype]['Value'].values\n",
    "    print(f\"{ftype:10s}: Mean={np.mean(vals):8.4f}, Std={np.std(vals):8.4f}, Min={np.min(vals):8.4f}, Max={np.max(vals):8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e98efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance and statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Feature Importance & Statistics Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 1: Top 20 features\n",
    "ax = axes[0, 0]\n",
    "top_20 = feature_df_sorted.head(20)\n",
    "colors_top = [{'MFCC': 'steelblue', 'Spectral': 'coral', 'Prosodic': 'lightgreen'}[t] for t in top_20['Type']]\n",
    "ax.barh(range(len(top_20)), top_20['Abs_Value'].values, color=colors_top, alpha=0.7, edgecolor='black')\n",
    "ax.set_yticks(range(len(top_20)))\n",
    "ax.set_yticklabels(top_20['Feature'].values, fontsize=9)\n",
    "ax.set_xlabel('Absolute Value')\n",
    "ax.set_title('Top 20 Most Important Features', fontsize=12, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Feature variance by type\n",
    "ax = axes[0, 1]\n",
    "types = ['MFCC', 'Spectral', 'Prosodic']\n",
    "variances = [feature_df[feature_df['Type'] == t]['Value'].std() for t in types]\n",
    "means = [feature_df[feature_df['Type'] == t]['Value'].mean() for t in types]\n",
    "x_pos = np.arange(len(types))\n",
    "ax.bar(x_pos, variances, color=['steelblue', 'coral', 'lightgreen'], alpha=0.7, edgecolor='black', linewidth=2)\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(types)\n",
    "ax.set_ylabel('Standard Deviation')\n",
    "ax.set_title('Feature Variance by Type', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (var, mean) in enumerate(zip(variances, means)):\n",
    "    ax.text(i, var, f'{var:.2f}\\n(Œº={mean:.2f})', ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Plot 3: Box plot by feature type\n",
    "ax = axes[1, 0]\n",
    "data_by_type = [feature_df[feature_df['Type'] == t]['Value'].values for t in types]\n",
    "bp = ax.boxplot(data_by_type, labels=types, patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], ['steelblue', 'coral', 'lightgreen']):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax.set_ylabel('Feature Value')\n",
    "ax.set_title('Feature Value Distribution by Type', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Feature count and summary stats\n",
    "ax = axes[1, 1]\n",
    "summary_text = f\"\"\"\n",
    "üìä FEATURE EXTRACTION SUMMARY\n",
    "\n",
    "Total Features: {len(complete_features)}\n",
    "\n",
    "Feature Breakdown:\n",
    "  ‚Ä¢ MFCC (52):      {np.mean(np.abs(mfcc_features)):.4f} avg magnitude\n",
    "  ‚Ä¢ Spectral (24):  {np.mean(np.abs(spectral_features)):.4f} avg magnitude  \n",
    "  ‚Ä¢ Prosodic (19):  {np.mean(np.abs(prosodic_features)):.4f} avg magnitude\n",
    "\n",
    "Statistics:\n",
    "  ‚Ä¢ Mean: {np.mean(complete_features):.4f}\n",
    "  ‚Ä¢ Std:  {np.std(complete_features):.4f}\n",
    "  ‚Ä¢ Min:  {np.min(complete_features):.4f}\n",
    "  ‚Ä¢ Max:  {np.max(complete_features):.4f}\n",
    "\n",
    "Key Insights:\n",
    "  ‚úì Features are in different scales\n",
    "  ‚úì Will need normalization before MLP\n",
    "  ‚úì Top features: {', '.join(feature_df_sorted.head(3)['Feature'].values)}\n",
    "\n",
    "Next Step:\n",
    "  Normalize features for neural network training\n",
    "\"\"\"\n",
    "ax.text(0.05, 0.95, summary_text, transform=ax.transAxes, fontsize=10, family='monospace',\n",
    "        verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Feature importance analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eec109",
   "metadata": {},
   "source": [
    "## Section 8: Feature Refinement & Customization\n",
    "\n",
    "**You can refine the feature extraction by:**\n",
    "\n",
    "1. **Adjusting MFCC Parameters** in `config.audio`:\n",
    "   - `N_MFCC`: Number of MFCC coefficients (default: 13)\n",
    "   - `N_MEL`: Number of mel bands (default: 128)\n",
    "   - `FMIN`, `FMAX`: Frequency range (default: 80-7600 Hz)\n",
    "\n",
    "2. **Adjusting Spectral Parameters** in `config.spectral`:\n",
    "   - Include/exclude specific spectral features\n",
    "   - Adjust statistics (mean, std, min, max)\n",
    "   - Modify chroma extraction\n",
    "\n",
    "3. **Adjusting Prosodic Parameters** in `config.prosodic`:\n",
    "   - Change F0 extraction method (librosa vs Parselmouth)\n",
    "   - Adjust jitter/shimmer window sizes\n",
    "   - Enable/disable specific prosodic markers\n",
    "\n",
    "4. **Feature Selection**:\n",
    "   - Use only MFCC (52 features) - simpler model\n",
    "   - Use MFCC + Spectral (76 features) - balanced\n",
    "   - Use all 106 features - comprehensive (current)\n",
    "   - Apply PCA to reduce to 80 features\n",
    "\n",
    "**Recommendation for your refinement:**\n",
    "Start with all 106 features to get baseline results. If you see overfitting, gradually:\n",
    "1. Reduce MFCC coefficients (13 ‚Üí 10)\n",
    "2. Remove highly correlated features\n",
    "3. Apply PCA dimensionality reduction\n",
    "4. Focus on ASD-specific markers (jitter, shimmer, F0 variation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature refinement guide and function\n",
    "def extract_features_with_options(audio, sr, use_mfcc=True, use_spectral=True, use_prosodic=True):\n",
    "    \"\"\"\n",
    "    Extract features with selective feature types.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    audio : array\n",
    "        Audio signal\n",
    "    sr : int\n",
    "        Sample rate\n",
    "    use_mfcc : bool\n",
    "        Include MFCC features\n",
    "    use_spectral : bool\n",
    "        Include spectral features\n",
    "    use_prosodic : bool\n",
    "        Include prosodic features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    features : array\n",
    "        Concatenated feature vector\n",
    "    feature_names : list\n",
    "        Feature names\n",
    "    feature_counts : dict\n",
    "        Count of each feature type used\n",
    "    \"\"\"\n",
    "    \n",
    "    features_list = []\n",
    "    names_list = []\n",
    "    counts = {'MFCC': 0, 'Spectral': 0, 'Prosodic': 0}\n",
    "    \n",
    "    # MFCC features\n",
    "    if use_mfcc:\n",
    "        mfcc_ext = MFCCExtractor(config)\n",
    "        mfcc_feat = mfcc_ext.extract(audio, sr)\n",
    "        features_list.append(mfcc_feat)\n",
    "        names_list.extend(mfcc_ext.get_feature_names())\n",
    "        counts['MFCC'] = len(mfcc_feat)\n",
    "    \n",
    "    # Spectral features\n",
    "    if use_spectral:\n",
    "        spec_ext = SpectralExtractor(config)\n",
    "        spec_feat = spec_ext.extract(audio, sr)\n",
    "        features_list.append(spec_feat)\n",
    "        names_list.extend(spec_ext.get_feature_names())\n",
    "        counts['Spectral'] = len(spec_feat)\n",
    "    \n",
    "    # Prosodic features\n",
    "    if use_prosodic:\n",
    "        prost_ext = ProsodicExtractor(config)\n",
    "        try:\n",
    "            prost_feat = prost_ext.extract(audio, sr)\n",
    "            features_list.append(prost_feat)\n",
    "            names_list.extend(prost_ext.get_feature_names())\n",
    "            counts['Prosodic'] = len(prost_feat)\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è  Prosodic extraction failed, skipping\")\n",
    "    \n",
    "    return np.concatenate(features_list), names_list, counts\n",
    "\n",
    "# Test different feature combinations\n",
    "print(\"üß™ TESTING FEATURE COMBINATIONS:\\n\")\n",
    "\n",
    "combinations = [\n",
    "    (True, False, False, \"MFCC only\"),\n",
    "    (True, True, False, \"MFCC + Spectral\"),\n",
    "    (True, False, True, \"MFCC + Prosodic\"),\n",
    "    (False, True, False, \"Spectral only\"),\n",
    "    (True, True, True, \"All features (default)\")\n",
    "]\n",
    "\n",
    "results = []\n",
    "for mfcc, spec, prost, label in combinations:\n",
    "    try:\n",
    "        feat, names, counts = extract_features_with_options(audio_processed, sr, mfcc, spec, prost)\n",
    "        total = len(feat)\n",
    "        results.append({\n",
    "            'Configuration': label,\n",
    "            'Total Features': total,\n",
    "            'MFCC': counts['MFCC'],\n",
    "            'Spectral': counts['Spectral'],\n",
    "            'Prosodic': counts['Prosodic']\n",
    "        })\n",
    "        print(f\"‚úì {label:30s} ‚Üí {total:3d} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó {label:30s} ‚Üí Error: {str(e)[:50]}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f05923",
   "metadata": {},
   "source": [
    "## Section 9: Summary & Next Steps\n",
    "\n",
    "### What You Learned in This Notebook:\n",
    "\n",
    "‚úÖ **Audio Preprocessing**: Standardize audio to 5 seconds, 16 kHz\n",
    "‚úÖ **MFCC Features (52)**: Speech spectrum with velocity & acceleration\n",
    "‚úÖ **Spectral Features (24)**: Frequency content & energy distribution  \n",
    "‚úÖ **Prosodic Features (19)**: Pitch, formants, voice quality\n",
    "‚úÖ **Feature Aggregation (106)**: Combining all features\n",
    "‚úÖ **Feature Analysis**: Understanding variance and importance\n",
    "‚úÖ **Feature Refinement**: Customizing for your needs\n",
    "\n",
    "### Key ASD/ADHD Markers:\n",
    "- **Jitter** (pitch instability) - ‚Üë in Autism\n",
    "- **Shimmer** (amplitude instability) - ‚Üë in Autism\n",
    "- **F0 Variation** (pitch range) - ‚Üì in Autism (monotone)\n",
    "- **Energy Patterns** - Irregular in ADHD\n",
    "- **Speech Rate** - Variations in ADHD\n",
    "\n",
    "### What's Next:\n",
    "\n",
    "**Phase 2: Data Preparation**\n",
    "- Load or generate training dataset\n",
    "- Create labeled samples (ASD, ADHD, Healthy)\n",
    "- Handle class imbalance\n",
    "\n",
    "**Phase 3: Model Training**\n",
    "- Build MLP classifier (106 ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 3)\n",
    "- Train with K-fold cross-validation\n",
    "- Monitor loss and accuracy\n",
    "\n",
    "**Phase 4: Model Evaluation**\n",
    "- Compute metrics (accuracy, precision, recall, F1)\n",
    "- Generate confusion matrix and ROC curves\n",
    "- Analyze misclassified samples\n",
    "\n",
    "**Phase 5: Refinement**\n",
    "- Test feature combinations\n",
    "- Adjust model architecture\n",
    "- Perform hyperparameter tuning\n",
    "\n",
    "### How to Use This Notebook:\n",
    "1. **Understanding Mode**: Run all cells to see feature extraction\n",
    "2. **Customization Mode**: Modify feature extractors and test\n",
    "3. **Reference Mode**: Use functions for your own audio files\n",
    "4. **Documentation**: Read markdown cells for theoretical background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a17f37",
   "metadata": {},
   "source": [
    "## Appendix: Reusing existing repository scripts and data\n",
    "\n",
    "This section shows how we can reuse the helper scripts and data already present in the repository root (`f:/AIML`) to speed up processing and to connect real datasets.\n",
    "\n",
    "We'll attempt to:\n",
    "- Detect and import high-value scripts: `mfcc_extract.py`, `extract_audio.py`, `ser_preprocessing.py`, `spectrogram_conversion.py`, `extractBERT.py`, `predictor.py`, `model.py`.\n",
    "- Demonstrate examples using functions from those scripts (if available).\n",
    "- Fall back to our in-notebook implementations when external scripts are missing or incompatible.\n",
    "\n",
    "Run the next cell to auto-detect available helpers and print a short usage summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect helper scripts in repository root and summarize their quick usage\n",
    "import importlib.util\n",
    "import inspect\n",
    "\n",
    "root_dir = r'f:/AIML'\n",
    "helper_files = [\n",
    "    'mfcc_extract.py',\n",
    "    'extract_audio.py',\n",
    "    'ser_preprocessing.py',\n",
    "    'spectrogram_conversion.py',\n",
    "    'extractBERT.py',\n",
    "    'predictor.py',\n",
    "    'model.py'\n",
    "]\n",
    "\n",
    "available_helpers = {}\n",
    "\n",
    "for hf in helper_files:\n",
    "    path = os.path.join(root_dir, hf)\n",
    "    if os.path.exists(path):\n",
    "        available_helpers[hf] = path\n",
    "\n",
    "print(f\"Detected {len(available_helpers)} helper scripts in {root_dir}:\")\n",
    "for k, v in available_helpers.items():\n",
    "    print(f\" - {k}: {v}\")\n",
    "\n",
    "# Try to import and list top-level functions/classes for each helper\n",
    "helper_summaries = {}\n",
    "for name, path in available_helpers.items():\n",
    "    spec = importlib.util.spec_from_file_location(name.replace('.py',''), path)\n",
    "    try:\n",
    "        module = importlib.util.module_from_spec(spec)\n",
    "        spec.loader.exec_module(module)\n",
    "        members = inspect.getmembers(module, predicate=inspect.isfunction)\n",
    "        classes = inspect.getmembers(module, predicate=inspect.isclass)\n",
    "        helper_summaries[name] = {\n",
    "            'functions': [m[0] for m in members],\n",
    "            'classes': [c[0] for c in classes]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        helper_summaries[name] = {'error': str(e)}\n",
    "\n",
    "print('\\nHelper script summaries:')\n",
    "for name, summary in helper_summaries.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    if 'error' in summary:\n",
    "        print(f\"  ‚ö†Ô∏è  Import error: {summary['error']}\")\n",
    "    else:\n",
    "        print(f\"  Functions: {summary['functions']}\")\n",
    "        print(f\"  Classes:   {summary['classes']}\")\n",
    "\n",
    "# Provide recommended next actions for each detected helper\n",
    "print('\\nRecommended quick actions:')\n",
    "for name in helper_summaries.keys():\n",
    "    if name == 'mfcc_extract.py':\n",
    "        print(' - Use mfcc_extract.extract_mfcc_features(filepath) to extract raw MFCC arrays and compare with our MFCCExtractor outputs.')\n",
    "    if name == 'extract_audio.py':\n",
    "        print(' - Use extract_audio.extract_audio_features to run openSMILE configs (if opensmile is available on your system).')\n",
    "    if name == 'ser_preprocessing.py':\n",
    "        print(' - Use ser_preprocessing.extract_features and ser_preprocessing.load_data() for ready-made pipelines; can replace synthetic dataset generation when real files are available in `data/` folder.')\n",
    "    if name == 'predictor.py' or name == 'model.py':\n",
    "        print(' - Inspect model/predictor for saved models and prediction helpers; can use for inference demonstration.')\n",
    "    if name == 'extractBERT.py':\n",
    "        print(' - Use extractBERT.extract_text_features(text) to generate textual embeddings for multimodal experiments.')\n",
    "    if name == 'spectrogram_conversion.py':\n",
    "        print(' - Use spectrogram_conversion code for converting long audio files to spectrogram images if you want image-based models.')\n",
    "\n",
    "print('\\nIf you want, I can automatically wire these helpers into the notebooks (example cells that call them).')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ASD_ADHD_Detection)",
   "language": "python",
   "name": "asd_adhd_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
